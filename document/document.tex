\documentclass[12pt, a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[left=40mm, right=20mm, top=30mm, bottom=30mm]{geometry}
\usepackage{titlesec}
\usepackage{xparse}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usetikzlibrary{shapes.geometric}

\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter}{20pt}{\Huge\bfseries}

\graphicspath{{figures/}}

\NewDocumentCommand{\codeword}{v}{%
\texttt{{#1}}%
}

\renewcommand*\contentsname{Table of Contents}

\title{Designing and implementing a tensor calculus}
\author{Farin Lippmann}
\date{August 2022}

% Area: Generic optimization
% Topic: Derivatives of higher-order tensor expressions
% Research Question: Can we design and implement an Einstein-notation-based tensor calculus for automatic symbolic differentiation of tensor expressions?

% Related Research: 
% - Matrix Calculus paper (uses ricci notation)
% - Tensor Calculus paper (basis for this thesis)
% - Algebra systems build scalar-valued functions for each tensor entry, inefficient [from Matrix Calculus Paper]

% Questions:
% - What am I improving over Matrix Calculus? 
%   -> Einstein notation instead of Ricci
%   -> Actually works on arbitrary tensor inputs
%   -> Cleanup inbetween derivation steps?
% - Why Einstein over Ricci?
%   -> Ricci is more complex, allows differentiating between tensors and their dual (multilinear maps)
%      which we don't need, tensors are strictly datastructures for us
%   -> Because we don't need the complexities of Ricci, Einstein allows a more elegant internal representation


\begin{document}
\maketitle

\chapter{Abstract}
\pagenumbering{gobble}

\chapter{Table of contents}
\pagenumbering{arabic}
\setcounter{page}{3}

\chapter{Introduction}

With the rise of machine learning applications, interest in optimization problems has been growing.
In these kinds of problems, one seeks to find values for a number of parameters that maximize or minimize a given scalar objective function.
While some optimization problems have a closed-form solution which can be computed directly, many can only be solved approximately.
When computing these approximate solutions, derivatives play a crucial role.
In particular, the first derivatives of the objective function are used in the ubiquitous gradient descent procedure.
In most optimization problems, multiple parameters need to be optimized simultaneously.
This means that derivatives need to be computed with respect to multiple variables, organized either in vectors, matrices or higher-order tensors.
Commonly used machine learning frameworks (like Tensorflow, PyTorch, Theano) focus heavily on the first derivatives of the objective function.

Higher-order derivatives can, however, also be of interest.
As an example, the matrix of second-order partial derivatives, called the Hessian, is used in the application of Newton's method, which can be more efficient than gradient descent for some problems.
Convexity checks constitute another application of higher-order derivatives.
The convexity of objective functions and their associated optimization problems play a major role in the field of optimization.
If a problem is strictly convex or concave, then it has a unique global optimum and the gradient descent algorithm can approximate it arbitrarily well.
To check for convexity, the objective function's Hessian matrix, the matrix of second-order partial derivatives, needs to be computed and checked for semidefiniteness.
If the function's inputs are naturally organized in a matrix or higher-order tensor, then the resulting Hessian will also be a higher-order tensor.
The prevalent machine learning frameworks have no way to directly compute these derivatives, and classical computer algebra systems struggle with efficiency, as they work on the level of individual tensor entries.

To bridge this gap, we design and implement a tensor calculus that allows for automatic symbolic differentiation of tensor expressions of any order.
Additionally, this calculus will use an Einstein-like notation to represent tensor products, rather than the more complex notation used in Ricci calculus.

This thesis builds on the work by Laue, Mitterreiter and Giesen, who have built a similar calculus for matrix derivatives [Matrix Calculus] and developed the theoretical foundation for an Einstein notation based tensor calculus [Tensor Paper].

The structure of the thesis is as follows.
First, the syntax of the calculus is developed in form of a grammar.
Then, the parsing and representation of tensor expressions are discussed, followed by a chapter on the differentiation process.

\chapter{A Syntax for Tensor Expressions}
\section{Aims}
In this chapter we develop the language that will be used to specify tensor expressions for differentiation.
To differentiate a tensor expression, three pieces of information are needed: 1. The tensor expression itself, 2. The variable with respect to which should be differentiated, 3. The tensor order of all variables in the expression.
The neccessity of the first two is obvious, while the third might not immediately be.
The orders of each part of the tensor expression need to be known because they play a crucial role in the differentiation rules that will later be implemented.

Our aims when developing the syntax are the following.
The syntax should allow representation of tensor expressions of arbitrary order.
It should be as general as possible, not focusing on one single interpretation of tensors.
Even so, as the motivation for this thesis comes from the background of optimization and machine learning, the most common specifics of these fields should be covered.

The main point of interest for this syntax is how to represent tensors and their products, as there are many ways to multiply two tensors.
We start with an established tensor calculus and its notation, and generalize it to fit our problem domain.

\section{Representing Tensor Products}
There exists a calculus for tensor expressions, called Ricci calculus, that is heavily used in physics.
It which allows differentiation between tensors as $n$-dimensional arrays and as multilinear functions.
The differentiation between the two forms occurs by the location of their indices, either in sub- or superscript.
For example, the inner product of two vectors, $y^Tx$, would be written as $y_ix^i$. And the Matrix-vector product $Ax$ would be $A^i_jx^j$.
While Ricci calculus can be useful, for example to physicists, the many indices on each tensor make it less readable.

Since, for our purposes, tensors are simply containers for variables, we have no need to expressly define some tensors as multilinear functions.
The main part of Ricci calculus that we bring over into our notation is a derivative of the Einstein summation convention.
This is a notational convention that allows elegant representation of tensor products.
Since tensors, depending on their order, may be multiplied in many ways, we need a way to describe which tensor axes should be multiplied in what way.
For example, two vectors could be multiplied by taking their inner product, their outer product, or even their elementwise product.
Einstein notation handles this problem by adding index sets to the inputs and the output of the product, each index representing an axis in the respective tensor.
If an index appears in both inputs, the respective axes will be multiplied.
If an index appears in inputs but not in the output, this axis will be summed over. (In true Einstein-notation, output indices are omitted. So indices that appear in both inputs are always summed over. For more generality, we include output indices.)
This simple set of rules provides an elegant way to express the main tensor operations.

We take a large part of the syntax for tensor multiplication from the established Python software package \textit{Numpy}, specifically it's \codeword{einsum} procedure.
A product of two tensors $x$ and $y$ has the form $x*_{(a,b \rightarrow c)}y$, where $a$,$b$ and $c$ are strings of indices.

As an example, see the inner, outer and elementwise product of two order 1 tensors (vectors) $x$ and $y$: $x*_{(i,i \rightarrow)}y$, $x*_{(i,j \rightarrow ij)}y$, $x*_{(i,i \rightarrow i)}y$.
The application of a matrix $A$ to a vector $v$ would be written as $A*_{(ij,j \rightarrow i)}v$, while the elementwise product of two matrices $A$ and $B$ would be $A*_{(ij,ij \rightarrow ij)}B$.
One may even take diagonals of tensors using this notation, the inner product of a square matrix $A$'s main diagonal and a vector $v$ would be $A*_{(ii,i \rightarrow)}v$.


\section{Grammar}
With the representation of tensor products handled, we can now define the full syntax grammar.
The grammar, presented here in extended backus-naur form, is based on a standard grammar for mathematical expressions by Julien Klaus.
It was adapted to tensor expressions by generalizing multiplication to tensor products using the einstein summation convention.
\begin{verbatim}
expr = term {( '+' | '-' ) term}+
term = factor {( '*(' productindices ')') factor | '/' factor}+
productindices = tensorindices ',' tensorindices '->' tensorindices
tensorindices = {smallalpha}+
factor = {'-'} atom {'^' ('(' expr ')' | atom)}+
atom = number | function '(' expr ')' | tensorname | '(' expr ')'
number = ['-'] digit* '.' digit* [( 'e' | 'E' ) ['+' | '-'] {digit}+]
digit = [0-9]
function = 'sin' | 'cos' | 'tan' | 'arcsin' | 'arccos' | 
    'arctan' | 'abs' | 'tanh' | 'exp' | 'log' | 'sign' | 
    'relu' | 'det' | 'inv' | 'adj'
\end{verbatim}
Here \texttt{\string{x\string}} describes that \texttt{x} may occur any number of times, while \texttt{(x)+} lets \texttt{x} appear at least once.

As can be seen in the grammar, we allow the following binary operations on tensors: products (\codeword{*(,->)}), sums (\codeword{+}), differences (\codeword{-}) and quotients (\codeword{/}).
With the exception of tensor products, which have already been discussed, these operations are executed elementwise.
Exponentiation (\codeword{^}) is also included, but not as a true binary operator on tensors, as the exponent may only be scalar, with each entry of the basis tensor being combined with the exponent.
This is not apparent from the grammar, as tensor variable order is specified separately from the tensor expression itself.

A variety of functions with relevance to optimization and machine learning are included, mostly being applied elementwise to tensors of any order.
The exceptions to this are the matrix inverse \codeword{inv} and the matrix determinant \codeword{det}, both of which are not applied elementwise and may only operate on order 2 tensors.
The inclusion of these functions takes away from the generality of our grammar, as the idea of an inverse and a determinant can only be applied to matrices, not higher-order tensors, but they are crucial to some common optimization problems.

\chapter{Parsing and Representation}
With the syntax handled, we may now discuss how a given tensor expression is parsed and represented internally.
In our application, tensor expressions are represented using directed acyclic graphs derived from binary trees.
This data structure was chosen because graphical representations are well tested for expressions and because the foundational paper by Laue, Mitterreiter and Giesen also assumes such a representation.
This allows the differentiation rules stated in the paper to be easily implemented.
Furthermore, many of the algorithms to be implemented later are most naturally written recursively.
A recursive data structure like a tree lends itself to these algorithms.
In this chaper we first describe how a tensor expression is parsed into a binary expression tree, then explain the processing steps taken to transform the binary tree into a directed acyclic graph without duplicate subexpressions.

\section{Parsing}
The process of turning a string into a different data structure that allows for further processing is called parsing.
It is split into two steps, tokenization and tree building.

\subsection{Tokenization}
To be able to parse an expression string, we first transform it into a sequence of tokens.
This is done by the tokenizer, sometimes also called lexical scanner, which recognizes certain defined patterns in the expression string and unites them into a token.
Each of these tokens represents a logical unit that is used in the next step of parsing.
Tokenization allows the parser to work only with well-defined tokens, not having to work on the level of individual characters.
Each token is made up of two parts, the descriptor and the identifier.
The descriptor is one of a few defined categories, describing what kind of logical unit the token forms, while the identifier contains the characters the token was formed from.
As an example, the string \codeword{A + sin(b)} would be tokenized into the following sequence: \codeword{(ALPHANUM, 'a'), (PLUS, '+'), (ELEMENTWISE_FUNCTION, 'sin'), (LRBRACKET, '('), (LOWERCASE_ALPHA, 'b'), (RRBRACKET, ')')}.
We define token descriptors for constants (e.g. \codeword{'1.05'}), variable names (e.g. \codeword{'A'}, \codeword{'b'}, \codeword{'C1'}), special symbols (\codeword{'+'}, \codeword{'-'}, \codeword{'*'}, \codeword{'/'}, \codeword{'^'}, \codeword{','}, \codeword{'('}, \codeword{')'}, \codeword{'>'}), keywords (\codeword{'declare'}, \codeword{'expression'}, \codeword{'derivative'}, \codeword{'wrt'}) and function names, which are recognized using the list of functions given in the grammar (e.g. \codeword{'sin'}, \codeword{'abs'}, \codeword{'det'}).
There are two token descriptors for functions, \codeword{ELEMENTWISE_FUNCTION} and \codeword{SPECIAL_FUNCTION}, because derivative rules are different for these two types.
Examples for special functions are the matrix determinant \codeword{det} and the matrix inverse \codeword{inv}.

\subsection{Tree Building}
The output of the tokenizer is used to build a binary tree representing the expression.

A binary tree is a type of tree where each node has exactly two child nodes.
Like in other trees, each node has exactly one parent node.
Exceptions to this are the root, which has no parent, and the leaves, which have no children.
Formally, we define a binary tree $B$ as follows:
$B$ is either the empty set or a tuple $(\text{left}(B),\text{root}(B),\text{right}(B))$, where $\text{left}(B)$ and $\text{right}(B)$ are also binary trees and have $B$ as their only parent, and $\text{root}(B)$ is the root of the tree, containing some value.

The process  of constructing such a tree from a given expression follows the structure of the grammar defined in the previous chapter.
Starting with \codeword{expr}, we recursively step through the grammar, using the tokens to decide which options to follow.
For each of the words defined in the grammar, there is a function in our parser.
When a certain word is expected, the function is called.
For example, at the start, \codeword{expr} is called, which first calls \codeword{term}, which calls \codeword{factor}, which then looks for any \codeword{MINUS}-tokens before calling \codeword{atom}, which then checks if there is a constant, a function application, a variable name, or another expression, and so forth.
Thus, we traverse the expression from left to right while building binary tree nodes when appropriate.
Leaf nodes are created when we reach constants or variable names, and they are connected using branch nodes containing operators or functions.
Each node, with its subtree, represents a subexpression, with the root node of the whole tree representing the entire expression.
If there is ever a point at which expectations are not met, for example, if a left round bracket token (\codeword{(LRBRACKET, '(')}) is expected but not found, an exception is thrown and the program aborted.

As an example, we take the expression from before, \codeword{A + sin(b)}, tokenized to \codeword{(ALPHANUM, 'a'), (PLUS, '+'), (ELEMENTWISE_FUNCTION, 'sin'), (LRBRACKET, '('), (LOWERCASE_ALPHA, 'b'), (RRBRACKET, ')')}.

The order of function calls generated when parsing this expression would be the following:

\begin{itemize}
    \setlength\itemsep{0.01em}
    \item \codeword{expr}
    \item \codeword{term}
    \item \codeword{factor}
    \item \codeword{atom} | We identify \codeword{'a'} as a variable name, generate a leaf node and jump back up the call stack to \codeword{expr}. There, we find the expected \codeword{'+'} and add a sum node as the parent of our leaf node.
    \item \codeword{term}
    \item \codeword{factor}
    \item \codeword{atom} | We identify \codeword{'sin'} as a function name and generate an elementwise-function node as the second child of our sum node. We also find the expected \codeword{'('}.
    \item \codeword{expr}
    \item \codeword{term}
    \item \codeword{factor}
    \item \codeword{atom} | We identify \codeword{'b'} as a variable name and generate another leaf node as the child of our elementwise-function node. We jump back up the call stack, find the expected \codeword{')'}, jump all the way up the call stack and finish.
\end{itemize}

The binary tree generated from this can be seen in Figure \ref{fig:tree_example}.
Note that, by convention, we set the argument of a unary function node to its right child.

\begin{figure}
    \centering
    \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
        % nodes
        \node[main] (A) at (0,2) {$A$};
        \node[main] (b) at (2,0) {$b$};
        \node[main] (+) at (1,4) {$+$};
        \node[main] (sin) at (2,2) {$\text{sin}$};
        % edges
        \draw [->, n] (A) -- (+);
        \draw [->, n] (b) -- (sin);
        \draw [->, n] (sin) -- (+);
    \end{tikzpicture}
    \caption{Binary expression tree generated from the expression string \texttt{A + sin(b)}. Empty nodes are omitted.}
    \label{fig:tree_example}
\end{figure}

\section{Preprocessing}
Having described how a binary expression tree is created from a given tensor expression, we now describe how this tree is processed further to prepare it for differentiation.
We employ the following three preprocessing steps.
\begin{itemize}
    \item Some subtrees are substituted with equivalent ones which are more easily differentiated.
    \item Subtrees which occur more than once are fused.
    \item Each node's output tensor order is determined and stored.
\end{itemize}

\subsection{Equivalent Subtree Substitution}
Some operations or functions may be viewed as equivalent to, or as a special case of another, more easily differentiable operation or function.
In these cases, we substitute the more general case to reduce the number of differentiation rules that need to be implemented.
For example, when encountering the expression $A - B$, we substitute $A + -(B)$, viewing $(-)$ as an elementwise function.
This allows us to use the differentiation rules for sums and elementwise functions, not having to implement a new rule for differences.
A full overview of the substituted expressions can be seen in Table \ref{tab:substitutions}.
\begin{table}[ht]
    \centering
    \begin{tabular}{l | l}
        Original & Substitute \\\hline
        $A-B$ & $A + -(B)$ \\
        $A^B$ & $\text{exp}(B \cdot \text{log}(A))$ \\
        $A/B$ & $A \;.^*\; \text{elementwise\_inverse}(B)$ \\
        $\text{adj(X)}$ & $\text{det}(X) *_{(,ij \rightarrow ij)} \text{inv}(X)$ \\ 
    \end{tabular}
    \caption{Table showing all operations that get substituted during preprocessing, along with their substitutes. Note that $(.^*)$ is a placeholder for elementwise multiplication and $(\cdot)$ is a placeholder for scalar multiplication. Both are replaced with the appropriate tensor products during tensor order determination. $\text{elementwise\_inverse}$ is an elementwise function for which a differentiation rule is implemented.}
    \label{tab:substitutions}
\end{table}
\FloatBarrier

\subsection{Common Subtree Elimination}
When differentiating, we need to be able to tell when a subexpression is reached multiple times.
To help with this, and also to reduce the number of nodes in the expression tree, we fuse subtrees that occur more than once.

First, to identify duplicate subtrees, the notion of equality needs to be defined for trees.
We define equality of two trees, $A$ and $B$, as follows.
$$
A = B \iff \text{root}(A) = \text{root}(B) \land \text{left}(A) = \text{left}(B) \land \text{right}(A) = \text{right}(B)
$$
where $\text{root}(X)$ refers to the root node of $X$, and $\text{left}(X)$, $\text{right}(X)$ to the left and right child trees of $X$, respectively.
Equality of nodes is handled by comparing all components of the node (node type, name, indices for product nodes) individually.

Now that we may identify when two subtrees are equal, we continue with common subtree elimination.
We first obtain all subtrees by traversing the whole tree.
Then, we store each unique subtree we find in a hash table.
When we encounter a subtree that is already present in our hashmap, we let its parent node point to the tree in the hashmap instead of the original.
In doing so, we remove duplicate subtrees and transform the binary expression tree into a directed acyclic graph (DAG) representing the expression.
Nodes in the DAG still only have two child nodes, but may have many parent nodes.

\begin{figure}
    \centering
    \begin{minipage}{7cm}
        \centering
        \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (x1) at (1,0) {$X$};
            \node[main] (11) at (3,0) {$1$};
            \node[main] (+1) at (3,2) {$+$};
            \node[main] (+2) at (4.5,4) {$+$};
            \node[main, shape=ellipse] (*) at (6,2) {$*_{(ij,ij \rightarrow ij)}$};
            \node[main] (x2) at (5,0) {$X$};
            \node[main] (+3) at (7,0) {$+$};
            \node[main] (x3) at (6,-2) {$X$};
            \node[main] (12) at (8,-2) {$1$};
            % edges
            \draw [->, n] (x1) -- (+1);
            \draw [->, n] (11) -- (+1);
            \draw [->, n] (+1) -- (+2);
            \draw [->, n] (*) -- (+2);
            \draw [->, n] (x2) -- (*);
            \draw [->, n] (+3) -- (*);
            \draw [->, n] (x3) -- (+3);
            \draw [->, n] (12) -- (+3);
        \end{tikzpicture}}
    \end{minipage}
    \begin{minipage}{7cm}
        \centering
        \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (x1) at (3.5,0) {$X$};
            \node[main] (11) at (0,0) {$1$};
            \node[main] (+1) at (0,2) {$+$};
            \node[main] (+2) at (2,6) {$+$};
            \node[main, shape=ellipse] (*) at (3.5,3.5) {$*_{(ij,ij \rightarrow ij)}$};
            % edges
            \draw [->, n] (x1) -- (+1);
            \draw [->, n] (11) -- (+1);
            \draw [->, n] (+1) -- (+2);
            \draw [->, n] (*) -- (+2);
            \draw [->, n] (x1) -- (*);
            \draw [->, n] (+1) -- (*);
        \end{tikzpicture}}
    \end{minipage}
    \caption{Left: Binary tree generated from the input expression \texttt{(X+1) + X*(ij,ij->ij) (X+1)}. Right: DAG obtained by eliminating common subtrees in the left tree.}
    \label{fig:tree_to_dag}
\end{figure}
\FloatBarrier
\subsection{Tensor order Determination}
To be able to accurately create new product nodes, which require knowledge of the tensor orders of both inputs and the output, we determine the output tensor order of all nodes in the expression DAG.
Starting at the bottom of the DAG, tensor orders are propagated upwards towards the root.
The bottom nodes contain either variables or constants.
The tensor orders of variables need to be given, as they function as the start for the upwards propagation of tensor orders.
Each parent node calculates its output tensor order based on its child nodes.
For example, a sum will have its output order set equal to the output order of its children.
A full overview of these order determination rules can be found in table \ref{tab:order_rules}.

So far we have not addressed how the tensor order of a constant is determined.
Constants may need to be broadcast to fit into the expression.
For example, in the expression \codeword{1 + det(X)} with an order 2 tensor \codeword{X}, \codeword{1} would be a scalar (order 0 tensor) since it needs to be summed with another scalar.
But in the expression \codeword{det(X + 1)}, \codeword{1} would need to be an order 2 tensor filled with ones, because it is summed with another order 2 tensor.
To accomplish this broadcasting, constants have their order set to $-1$ to start with.
Then, when an error is encountered during tensor order determination because an order $-1$ constant does not fit into a part of the expression, the order of the constant is set to the value that makes it fit.

In a sum, this would be the order of the other summand.
In a product, the order would be determined by the given input indices.
In a special function, the order is the input order the function operates on. Since all our special functions operate on matrices, this is always 2.
The case of an elementwise function is more difficult, since they operate on arbitrary input tensor orders.
Nonetheless, we may find a fitting order using the expression in which the output of the elementwise function is used, by using the order that is needed there.
This is possible since the input and output order of elementwise functions are always the same.
As an example, the order of the tensor constant \codeword{2} in the expression \codeword{X + log(2)} may be determined by finding out what order \codeword{log(2)} needs to be to fit in the sum.
Since summands always have the same order, \codeword{log(2)}, and by extension \codeword{2}, needs to have the same order as \codeword{X}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l | l}
        Parent Node Type & Output order \\\hline
        \codeword{SUM} & $\text{order}(\text{left}(X))$ \\
        \codeword{PRODUCT} & $\text{output\_indices}(X)$ \\
        \codeword{ELEMENTWISE FUNCTION} & $\text{order}(\text{right}(X))$ \\
        \codeword{adj}, \codeword{inv} & $2$ \\
        \codeword{det} & $0$ \\
    \end{tabular}
    \caption{Rules applied to determine tensor orders of a node $X$, depending on the type of node it is. Note that the $\text{output\_indices}$ of a product node are given, that the sum may also take its order from the right child, as it has the same order, and that each special function (\texttt{det}, \texttt{inv}, \texttt{adj}) needs its own rule.}
    \label{tab:order_rules}
\end{table}

% TODO: Write about axis propagation (why it's necessary and how it's done)

\FloatBarrier
\chapter{Differentiation}
The last chapter was concerned with creating a suitable internal representation for tensor expressions, which turned out to be a directed acyclic graph (DAG).
In this chapter, we use this DAG representation of the expression for the differentiation.
We start by giving some background information on the general differentiation procedure and describe the differentiation rules applied in theory.
Following that, we elaborate on the details of our implementation of the procedure and rules.

\section{Background}
The theory for differentiating tensor expressions using our Einstein-like notation was developed by Laue, Mitterreiter and Giesen.
They developed differentiation rules for DAG representations of tensor expressions based on the Fr\'{e}chet Derivative.
In their paper, they present two modes of differentiation, forward and reverse mode.
This thesis utilizes reverse mode. %TODO: Warum reverse mode statt forward mode?

In reverse mode, the computation traverses the DAG from top to bottom.
The goal is to calculate for each node $v$ the derivative of the whole expression $y$ with respect to $v$.
This value, $\frac{dy}{dv}$, is also called the pullback of $v$.
Since the pullback is known for each parent node $u$ of $v$, we may apply the chain rule:

\begin{equation}
    \frac{dy}{dv} = \sum_{u: (v,u) \in E} \frac{dy}{du} \cdot \frac{du}{dv}
    \label{eq:diff}
\end{equation}

where $(v,u) \in E$ means that there is an edge going from $v$ to $u$ in the DAG, which makes $u$ a parent of $v$. 
Note also that $\cdot$ is a placeholder for an appropriate tensor product, which is specified in the differentiation rules. 
This rule allows us to calculate the pullback of $v$ using the known pullbacks of $v$'s parents $u$.
The only new calculation to be done for each node is the derivation of the parent nodes $u$ with respect to $v$, $\frac{du}{dv}$.

This computation starts with the topmost node $y$, representing the entire expression, and computes its pullback, which is the derivative with respect to itself.
This pullback is then used to compute the pullback of $y$'s child nodes, which are used to compute their child nodes, continuing recursively until all paths from the top node $y$ to the argument node have been traversed.
The pullback for the node containing the argument is the result of the differentiation, the derivative of the whole expression $y$ with respect to the argument.

In the very first step of the procedure, the derivative of the top node $y$ with respect to itself is computed.
Since $y$ is a tensor-valued function, its derivative is also a tensor.
Specifically, differentiating $y$ with respect to itself means that each entry of the tensor represented by $y$ is differentiated once with respect to all other entries.
Thus, this differentiation creates a new tensor of the same size as $y$ for each entry in $y$, where for each of these tensors, only one entry is $1$, while the rest are $0$.
Concretely, if $y$ is an order $n$ tensor, then it's derivative is a tensor $\delta_n$ of order $2n$, where
\begin{equation}
    \delta_n[i_1,..,i_{2n}] = 
    \begin{cases}
        1 \quad \text{if} \; (i_1,..,i_n) = (i_{n+1}...i_{2n}) \\
        0 \quad \text{else}
    \end{cases}
    \label{eq:delta}
\end{equation}
\begin{figure}
    $$
    \begin{pmatrix}
        x_1\\
        x_2\\
        x_3
    \end{pmatrix} \quad \quad \quad \quad 
    \begin{pmatrix}
        \frac{\partial x_1}{\partial x_1} & \frac{\partial x_2}{\partial x_1} & \frac{\partial x_3}{\partial x_1} \\
        \frac{\partial x_1}{\partial x_2} & \frac{\partial x_2}{\partial x_2} & \frac{\partial x_3}{\partial x_2} \\
        \frac{\partial x_1}{\partial x_3} & \frac{\partial x_2}{\partial x_3} & \frac{\partial x_3}{\partial x_3}
    \end{pmatrix}
    =
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    $$
    \caption{Left: An order 1 tensor. Right: The derivative of the left tensor with respect to itself, resulting in an order 2 delta tensor.}
\end{figure}
Intuitively, the first $n$ indices point to the entry of $y$ that is being differentiated, while the last $n$ indices point to the entry with respect to which we are differentiating.
Tensors that satisfy Equation \ref{eq:delta} are called delta tensors.
Note that we call a delta tensor of order $2n$ by the name $\delta_n$ and not $\delta_{2n}$, because delta tensors may only have even order.
The first node created in the differentiation procedure is always a delta tensor representing the topmost node $y$ differentiated with respect to itself.

\section{Differentiation Rules}
Having established the basic derivation procedure, we will now describe the rules used to compute the derivative $\frac{du}{dv}$ for a node $v$ and its parent $u$.
Proofs for the correctness of these rules can be found in the paper by Laue, Mitterreiter and Giesen %TODO: REFERENCE HERE.
Since we are computing $\frac{du}{dv}$, the applied rule depends on the type of node $u$ is.

\subsection{Sum Rule}
If $u$ is a sum node, the derivation is very simple.
In this case, $u$ adds nothing of its own to the pullback of $v$, so the contribution of $u$ to the pullback of $v$ is only $\frac{dy}{du}$.
Thus, when differentiating a sum node that is their childrens only parent, like the one seen in Figure \ref{fig:sum_rule}, the pullback of the children is simply the pullback of the sum node.

\begin{figure}
    \centering
    \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[]  (dots) at (1,4) {...};
            \node[main] (A) at (0,0) {$A$};
            \node[main] (X) at (2,0) {$X$};
            \node[main] (+) at (1,2) {$+$};
            % edges
            \draw [->, n] (A) -- (+);
            \draw [->, n] (X) -- (+);
            \draw [->, n] (+) -- (dots);
        \end{tikzpicture}
    }
    \caption{A sum node in an expression DAG. The pullbacks of its children $A$ and $X$ are the same as its pullback.}
    \label{fig:sum_rule}
\end{figure}

\subsection{Product Rule}
If $u$ is a product node representing the subexpression $v *_{(s_1,s_2 \rightarrow s_3)} w$ in a tensor expression where the topmost node has indices $s4$, then we may apply the product rule as follows.
The contribution of $u$ to the pullback of $v$ is 
$$
\frac{dy}{du} *_{(s_4s_3,s_2 \rightarrow s_4s_1)} w,
$$
and the contribution to the pullback of $w$ is 
$$
\frac{dy}{du} *_{(s_4s_3,s_1 \rightarrow s_4s_2)} v.
$$
An example of how the product rule is applied to an expression DAG can be seen in Figure \ref{fig:prod_rule}.
There is a caveat to the product rule, because by itself it is not applicable to all tensor products.
This detail will be discussed further in the section on implementation details.

\begin{figure}
    \centering
    \begin{minipage}{7cm}
        \centering
        \scalebox{0.8}{
            \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
                % nodes
                \node[]  (dots) at (1,4) {...};
                \node[main] (a) at (0,0) {$a$};
                \node[main] (x) at (2,0) {$x$};
                \node[main, ellipse] (*) at (1,2) {$*_{(, i \rightarrow i)}$};
                % edges
                \draw [->, n] (a) -- (*);
                \draw [->, n] (x) -- (*);
                \draw [->, n] (*) -- (dots);
            \end{tikzpicture}
        }
    \end{minipage}
    \begin{minipage}{7cm}
        \centering
        \scalebox{0.8}{
            \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
                % nodes
                \node[]           (dots) at (0,0) {...};
                \node[main, ellipse] (*) at (1,2) {$*_{(i, \rightarrow i)}$};
                \node[main]          (a) at (2,0) {$a$};
                % edges
                \draw [->, n] (dots) -- (*);
                \draw [->, n] (a) -- (*);
            \end{tikzpicture}
        }
    \end{minipage}
    \caption{\textbf{Left}: A product node representing a scalar-vector-product in an expression DAG. "..." represents the rest of the DAG. \textbf{Right}: Derivative of the left DAG with respect to $x$ computed using the product rule. "..." represents the pullback of the product node. Note that the product indices in the right DAG depend on the indices $s4$ of the topmost node in the left DAG (not visible here), which is assumed to be scalar in this case.}
    \label{fig:prod_rule}
\end{figure}

\subsection{Function Rule}
If $u$ is a function node, we distinguish between two cases.
The node $u$ either represents an elementwise function or a special function.
The elementwise case allows for a slight simplification over the special function case.

If the node $u$ represents an elementwise function $f$ being applied to the child node $v$, then the contribution of $u$ to the pullback of $v$ is 
$$
\frac{dy}{du} *_{(s_2s_1,s_1 \rightarrow s_2s_1)} f'(v),
$$
where $s_1$ is the index string of $v$, $s_2$ is the index string of the output node of the DAG and $f'$ is the derivative of $f$.

If the node $u$ represents a special function $f$ being applied to the child node $v$, then the contribution of $u$ to the pullback $v$ is
$$
\frac{dy}{du} *_{(s_3s_2,s_2s_1 \rightarrow s_3s_1)} f'(v),
$$
where $s_1$ is $v$'s input, representing the domain of $f$, $s_2$ is the index string of $v$, representing the range of $f$, and $s_3$ is the index string of the output node of the DAG with $f'$ still being the derivative of $f$.
The distinction between $s_1$ and $s_2$ is neccessary because domain and range of $f$ may not be the same with elementwise functions.

The derivatives $f'$ of all accepted functions $f$ need to be known.
They are listed in Table \ref{tab:func_diffs}
\begin{table}[ht]
    \centering
    \begin{tabular}{l | l}
        Function & Derivative wrt. x \\\hline
        $-(x)$ & $-(1)$ \\
        $\text{sin}(x)$ & $\text{cos}(x)$ \\
        $\text{cos}(x)$ & $-(\text{sin}(x))$ \\
        $\text{tan}(x)$ & $1 ./ (\text{cos}(x) .^* \text{cos}(x))$ \\
        $\text{arcsin}(x)$ & $1 ./ (1 + -(x .^* x))^{0.5}$ \\
        $\text{arccos}(x)$ & $-(1 ./ (1 + -(x .^* x))^{0.5})$ \\
        $\text{arctan}(x)$ & $1 ./ (1 + (x .^* x))$ \\
        $\text{exp}(x)$ & $\text{exp}(x)$ \\
        $\text{log}(x)$ & $1 ./ x$ \\
        $\text{tanh}(x)$ & $1 + -(\text{tanh}(x) .^* \text{tanh}(x))$ \\
        $\text{abs}(x)$ & $\text{sign}(x)$ \\
        $\text{sign}(x)$ & $0$ \\
        $\text{relu}(x)$ & $\text{relu}(\text{sign}(x))$ \\
        $1 ./ x$ & $- (1 ./ (x .^* x))$ \\
        $\text{inv}(x)$ & $-(\text{inv}(x)) *_{(ij,kl \rightarrow iklj)} \text{inv}(x)$ \\
        $\text{det}(x)$ & $\text{adj}(x) *_{(ij, \rightarrow ji)} 1$ \\
    \end{tabular}
    \caption{Derivatives of all used functions. $.^*$ is a placeholder for elementwise multiplication with the fitting indices. The elementwise inverse $1 ./ x$ is internally handled as an elemenwise function, which is why its derivative is listed here.} %TODO: Add a source?
    \label{tab:func_diffs}
\end{table}
\FloatBarrier


\begin{figure}
    \centering
    \begin{minipage}{7cm}
        \centering
        \scalebox{0.8}{
            \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
                % nodes
                \node[]    (dots) at (0,4) {...};
                \node[main] (sin) at (0,2) {$\text{sin}$};
                \node[main] (x) at (0,0) {$X$};
                % edges
                \draw [->, n] (x) -- (sin);
                \draw [->, n] (sin) -- (dots);
            \end{tikzpicture}
        }
    \end{minipage}
    \begin{minipage}{7cm}
        \centering
        \scalebox{0.8}{
            \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
                % nodes
                \node[]           (dots) at (0,0) {...};
                \node[main, ellipse] (*) at (1,2) {$*_{(ij, ij \rightarrow ij)}$};
                \node[main]          (cos) at (2,0) {$\text{cos}$};
                \node[main]          (x) at (2,-2) {$X$};
                % edges
                \draw [->, n] (dots) -- (*);
                \draw [->, n] (cos) -- (*);
                \draw [->, n] (x) -- (cos);
            \end{tikzpicture}
        }
    \end{minipage}
    \caption{\textbf{Left}: Part of a DAG that contains the subexpression $\text{sin(X)}$. $X$ is an order 2 tensor (matrix) and "..." represents the rest of the DAG. \textbf{Right}: Derivative of the left DAG with respect to $X$ computed using the rule for elementwise functions. "..." represents the pullback of the function node. Note that the topmost output node of the left DAG is assumed to be scalar here, which has an impact on the indices in the product node in the right DAG.}
    \label{fig:func_rule}
\end{figure}

\FloatBarrier
\section{First Example}
In this section, we present a detailed example of the differentiation procedure.
Consider the simple DAG in Figure \ref{fig:diff_example_original}, representing the expression $X *_{(ij,ij \rightarrow ij)} X$, the elementwise product of a matrix $X$ with itself.
We will be differentiating this DAG with respect to $X$.
Each step of the process is visualized in Figure \ref{fig:diff_example_steps}.

Following the differentiation procedure, we start by differentiating the topmost node with respect to itself.
Since the expression is an order-2 tensor, this creates a delta tensor $\delta_2$ of order 4 (step 1 in Figure \ref{fig:diff_example_steps}).
Continuing from the topmost node, we (arbitrarily) compute its contribution to the pullback of the left child first.
In this case, the left child and the right child are the same, but are still handled separately.
Using the product rule, we can see that the contribution of the top node $y$ to the left child is $\frac{dy}{dy} *_{(abij,ij \rightarrow abij)} X$, with $\frac{dy}{dy}$ being $\delta_2$ (step 2 in Figure \ref{fig:diff_example_steps}).
Since $X$ has no children, we continue with the right child of $y$.
Here, we apply the product rule and obtain the same $\delta_2 *_{(abij,ij \rightarrow abij)} X$ as the contribution to the pullback of $X$.
As contributions to the pullbacks of the same node need to be added (see Equation \ref{eq:diff}), we add a sum node as the root of our differentiation DAG (step 3 in Figure \ref{fig:diff_example_steps}).
Because all ways from the root node to the argument have been processed, we are finished.
The final DAG, simplified using common subtree elimination, can be seen in Figure \ref{fig:diff_example_final}.

\begin{figure}
    \centering
    \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (X) at (0,0) {$X$};
            \node[main, ellipse] (*) at (0,2) {$*_{(ij,ij \rightarrow ij)}$};
            % edges
            \draw [->, n] (X) to[out=145, in=-135] (*);
            \draw [->, n] (X) to[out=35, in=-45] (*);
        \end{tikzpicture}
    }
    \caption{Expression DAG representing the elementwise product of a matrix $X$ with itself.}
    \label{fig:diff_example_original}
\end{figure}

\begin{figure}
    \centering
    \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (1delta1) at (0,0) {$\delta_2$};

            \node[] (arr11) at (1.5,1) {};
            \node[] (arr12) at (2.5,1) {};

            \node[main] (1delta1) at (4,0) {$\delta_2$};
            \node[main] (1x1) at (6,0) {$X$};
            \node[main, ellipse] (1*1) at (5,2) {$*_{(abij,ij \rightarrow abij)}$};

            \node[] (arr21) at (7.5,1) {};
            \node[] (arr22) at (8.5,1) {};

            \node[main] (3delta1) at (10,0) {$\delta_2$};
            \node[main] (3x1) at (12,0) {$X$};
            \node[main, ellipse] (3*1) at (11,2) {$*_{(abij,ij \rightarrow abij)}$};
            \node[main] (3delta2) at (14,0) {$\delta_2$};
            \node[main] (3x2) at (16,0) {$X$};
            \node[main, ellipse] (3*2) at (15,2) {$*_{(abij,ij \rightarrow abij)}$};
            \node[main] (3+) at (13,4) {$+$};
            % edges
            \draw [->, n] (1delta1) -- (1*1);
            \draw [->, n] (1x1) -- (1*1);

            \draw [->, n] (3delta1) -- (3*1);
            \draw [->, n] (3x1) -- (3*1);
            \draw [->, n] (3x2) -- (3*2);
            \draw [->, n] (3delta2) -- (3*2);
            \draw [->, n] (3*1) -- (3+);
            \draw [->, n] (3*2) -- (3+);

            \draw [->, n] (arr11) -- (arr12);
            \draw [->, n] (arr21) -- (arr22);
        \end{tikzpicture}
    }
    \caption{Progression of the differentiation procedure on the DAG in Figure \ref{fig:diff_example_original}. Left: Pullback of the topmost node. Middle: One contribution to the pullback of $X$. Right: Pullback of $X$, both contributions summed.}
    \label{fig:diff_example_steps}
\end{figure}

\begin{figure}
    \centering
    \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (1delta1) at (0,0) {$\delta_2$};
            \node[main] (1x1) at (2,0) {$X$};
            \node[main, ellipse] (1*1) at (1,2) {$*_{(abij,ij \rightarrow abij)}$};
            \node[main] (+) at (1,4) {+};
            % edges
            \draw [->, n] (1delta1) -- (1*1);
            \draw [->, n] (1x1) -- (1*1);
            \draw [->, n] (1*1) to[out=135, in=-135] (+);
            \draw [->, n] (1*1) to[out=45, in=-45] (+);
        \end{tikzpicture}
    }
    \caption{Differentiation DAG of the original DAG in Figure \ref{fig:diff_example_original}, simplified using common subtree elimination.}
    \label{fig:diff_example_final}
\end{figure}

\FloatBarrier
\section{Implementation}
In this chapter we describe how the reverse mode differentiation procedure is implemented using the DAG representation of tensor expressions and the differentiation rules introduced in the previous sections.

An overview of the differentiation procedure can be seen in Algorithm \ref{alg:1} and Algorithm \ref{alg:2}.
The details will be explained in the following paragraphs.

\begin{algorithm}
    \caption{Differentiate (\textit{root}, \textit{argument})}\label{alg:1}
    \begin{algorithmic}
        \State \textit{diffdag} $\gets$ Node(delta(order(\textit{root})))
        \State ReverseModeDiff(\textit{root}, \textit{argument}, \textit{diffdag}, EmptyDictionary())
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{ReverseModeDiff (\textit{node}, \textit{argument}, \textit{diffdag}, \textit{contributions})}\label{alg:2}
    \begin{algorithmic}
        \For{each \textit{child} of \textit{node}}
            \If{subtree of \textit{child} does not contain \textit{argument}}
                \State Continue with next \textit{child}
            \EndIf
            \State Apply fitting differentiation rule (depending on \textit{node}) to \textit{child}
            \If{\textit{child} is in keys(\textit{contributions})}
                \State Add new contribution to \textit{contributions}[\textit{child}] using a sum node
                \State Move all parents of the \textit{contributions}[\textit{child}] to the sum node
                \State Update \textit{contributions}[\textit{child}] as the sum node
            \Else
                \State \textit{contributions}[\textit{child}] $\gets$ \textit{diffdag}
                \State \textit{diffdag} $\gets$ ReverseModeDiff(\textit{child}, \textit{argument}, \textit{diffdag}, \textit{contributions})
            \EndIf
        \EndFor \\
        \Return diffdag
    \end{algorithmic}
\end{algorithm}

As can be seen in Algorithm \ref{alg:1}, the first node is added specially, before the real differentiation algorithm starts.
This node, a delta tensor, is the derivative of the top node $y$ with respect to itself and has double the order of the original top node.

The original DAG is traversed from the top node down to the argument node, building up the differentiation DAG alongside it.
With each node $u$ of the original DAG that is processed, its contributions to the pullbacks of its child nodes are added to the differentiation DAG.
These pullbacks are computed using the differentiation rules discussed in the previous chapter.

As we are operating on a DAG and not a tree, a single node may be reached multiple times during differentiation.
When differentiating a node $u$, one of its children $v$ may have already been reached through a different parent.
In this case, a part of the pullback of $v$ has already been computed and is present in the differentiation DAG.
The new pullback contribution we computed from $u$ needs to be added to the old part, as can be seen from Equation \ref{eq:diff}.
To be able to do this, we keep a dictionary mapping from nodes in the original DAG which we have already reached to their pullback in the differentiation DAG.
This lets us add the old and new pullback contributions with a sum node.
Afterwards, we update the entry in the dictionary to the sum node that was just added.
We also need to move all nodes in the differentiation DAG that rely on the pullback of $v$, which are its parents.
Thus, we set all parents of the old pullback to have the updated pullback as their child.
This way, we build up the correct pullback over time, adding contributions as we find new ways to reach each node.

\section{Second Example}
To better illustrate the process of building a pullback piece by piece, we showcase an example.
Consider the DAG in Figure \ref{fig:alg_example_1} representing the expression $\text{sin}(v) + -(\text{sin}(v))$ for an order 1 tensor $v$.
We differentiate this expression with respect to $v$.

Starting out, we compute the pullback of the topmost node.
Since the output of the topmost node is an order 1 tensor, its pullback, the derivative with respect to itself, is an order 2 delta tensor $\delta_1$. (Step 1 in Figure \ref{fig:alg_example_2})

Next, we compute the contribution of the topmost node to its children's pullbacks.
However, since the topmost node is a sum node, it adds nothing of its own, which makes its childrens pullbacks the same as its.

Continuing with the node's children, we start (arbitrarily) with the left child.
The left child is an elementwise function node representing $\text{sin}(v)$.
Using the differentiation rule for elementwise functions along with the derivative of the $\text{sin}$ function, we see that its contribution to its child's pullback is $\delta_1 *_{(ba,a \rightarrow ba)} \text{cos}(v)$.
This is the first contribution to the pullback of $v$. (Step 2 in Figure \ref{fig:alg_example_2})
Since the only child of the $\text{sin}$ node is the argument node $v$, we need not continue further and instead jump back up to explore the second child of the topmost node.

The second child is an elementwise function node representing $-(\text{sin}(v))$.
Using the differentiation rule for elementwise functions along with the derivative of the $(-)$ function, we get $\delta_1 *_{(ba,a \rightarrow ba)} -1$ as the contribution of the node to its child's pullback.

The node's child is the $\text{sin}$ node we have been to before.
This means we need to add the new contribution to this node's pullback to the old one.
The old contribution to the pullback of $\text{sin}(v)$ was $\delta_1$, to which we now add $\delta_1 *_{(ba,a \rightarrow ba) -1}$ using a sum node.
Since the pullback of $v$ depends on the pullback of $\text{sin}(v)$, we need to make sure to update it.
We manage this by moving all parents of the old pullback to be parents of the new pullback instead.
The left child of the product $\delta_1 *_{(ba,a \rightarrow ba)} \text{cos}(v)$ gets updated to be the sum node in the new, complete pullback. (Step 3 in Figure \ref{fig:alg_example_2})

Since there are no more unvisited children, we are done.
The finished differentiation DAG represents the expression $(\delta_1 + (\delta_1 *_{(ba,a \rightarrow ba)} -1)) *_{(ba,a \rightarrow ba)} cos(v)$.
The complete, simplified DAG is visible in Figure \ref{fig:alg_example_3}.

\section{The Problematic Case of Disappearing Indices}
In this section we present a problem with the differentiation of tensor products that came up during development, along with a solution.
% TODO: Write about disappearing indices (when and why does it happen, how did we solve it)

\begin{figure}
    \centering
    \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (v) at (0,0) {$v$};
            \node[main] (sin) at (0,2) {$\text{sin}$};
            \node[main] (-) at (1.5,3) {-};
            \node[main] (+) at (0,4) {+};
            % edges
            \draw [->, n] (v) -- (sin);
            \draw [->, n] (sin) -- (+);
            \draw [->, n] (sin) -- (-);
            \draw [->, n] (-) -- (+);
        \end{tikzpicture}
    }
    \caption{DAG representing the expression $\text{sin}(v) + -(\text{sin}(v))$.}
    \label{fig:alg_example_1}
\end{figure}

\begin{figure}
    \centering
    \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (1delta) at (0,2) {$\delta_1$};

            \node[] (arr11) at (1.5,2) {};
            \node[] (arr12) at (2.5,2) {};

            \node[main] (2v) at (6,0) {$v$};
            \node[main] (2cos) at (6,2) {$\text{cos}$};
            \node[main] (2delta) at (4,2) {$\delta_1$};
            \node[main, ellipse] (2*1) at (5,4) {$*_{(ba,a \rightarrow ba)}$};

            \node[] (arr21) at (7.5,2) {};
            \node[] (arr22) at (8.5,2) {};
            
            \node[main] (3v) at (14.2,0) {$v$};
            \node[main] (3cos) at (14.2,2) {$\text{cos}$};
            \node[main, ellipse] (3*1) at (13.2,4) {$*_{(ba,a \rightarrow ba)}$};
            \node[main] (3+) at (12,2) {$+$};
            \node[main, ellipse] (3*2) at (12,0) {$*_{(ba,a \rightarrow ba)}$};
            \node[main] (3-1) at (13,-2) {$-1$};
            \node[main] (3delta1) at (11,-2) {$\delta_1$};
            \node[main] (3delta2) at (9.8,0) {$\delta_1$};
            % edges
            \draw [->, n] (2v) -- (2cos);
            \draw [->, n] (2cos) -- (2*1);
            \draw [->, n] (2delta) -- (2*1);

            \draw [->, n] (3v) -- (3cos);
            \draw [->, n] (3cos) -- (3*1);
            \draw [->, n] (3+) -- (3*1);
            \draw [->, n] (3*2) -- (3+);
            \draw [->, n] (3-1) -- (3*2);
            \draw [->, n] (3delta1) -- (3*2);
            \draw [->, n] (3delta2) -- (3+);

            \draw [->, n] (arr11) -- (arr12);
            \draw [->, n] (arr21) -- (arr22);
        \end{tikzpicture}
    }
    \caption{Progression of the differentiation procedure on the DAG in Figure \ref{fig:alg_example_1}. Left: Pullback of the $+$, $\text{sin}$ and $(-)$ nodes. Middle: First contribution to the pullback of $v$. Right: Full pullback of $v$.}
    \label{fig:alg_example_2}
\end{figure}

\begin{figure}
    \centering
    \scalebox{0.8}{
        \begin{tikzpicture}[main/.style={draw, circle, minimum size=30pt, line width=2pt}, info/.style={rectangle, rounded corners=5, draw}, n/.style={-latex, line width=0.75pt}, d/.style={-latex, line width=0.75pt, dashed}]
            % nodes
            \node[main] (v) at (2.2,0) {$v$};
            \node[main] (cos) at (2.2,2) {$\text{cos}$};
            \node[main, ellipse] (*1) at (1.2,4) {$*_{(ba,a \rightarrow ba)}$};
            \node[main] (+) at (0,2) {$+$};
            \node[main, ellipse] (*2) at (0,0) {$*_{(ba,a \rightarrow ba)}$};
            \node[main] (-1) at (0,-2) {$-1$};
            \node[main] (delta) at (-2,-2) {$\delta_1$};
            % edges
            \draw [->, n] (v) -- (cos);
            \draw [->, n] (cos) -- (*1);
            \draw [->, n] (+) -- (*1);
            \draw [->, n] (*2) -- (+);
            \draw [->, n] (-1) -- (*2);
            \draw [->, n] (delta) -- (*2);
            \draw [->, n] (delta) to[out=90, in=220] (+);
        \end{tikzpicture}
    }
    \caption{}
    \label{fig:alg_example_3}
\end{figure}

\chapter{Literature}
\chapter{Appendices}
\chapter{Declaration of autonomy}

\end{document}